{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d582d91d",
      "metadata": {
        "id": "d582d91d"
      },
      "source": [
        "# Tâche 2 - Modèles de langue N-grammes - Comme le disait le proverbe...\n",
        "\n",
        "L'objectif de cette tâche est de construire un modèle de langue Ngrammes de proverbes et de l'utiliser pour accomplir 2 sous-tâches. La première consiste à déterminer, parmi les choix que nous vous offrons, lequel correspond à un proverbe connu. Cette épreuve vise à évaluer la capacité des modèles unigrammes, bigrammes et trigrammes à bien estimer la vraisemblance d'un passage de texte.  \n",
        "\n",
        "La deuxième sous-tâche consiste à utiliser les modèles N-grammes comme des générateurs de textes et à compléter des proverbes. On vous donne une liste de proverbes se terminant par des mots masqués. L'épreuve consiste à générer un nombre de mots équivalent au nombre de masques et à faire la substitution des masques par les mots.\n",
        "\n",
        "Voir l'énoncé du travail #1 pour une description plus détaillée de cette tâche et des 2 sous-tâches.\n",
        "\n",
        "Les fichiers à utiliser pour cette tâche :\n",
        "- *t2_proverbes.txt*: il contient plus de 3000 proverbes, un par ligne de texte. Vous utilisez ce fichier pour l'entraînement des modèles de langues N-grammes.\n",
        "- *t2_proverbes_test1.json*: il contient des listes choix de réponses, chaque liste contenant un seul proverbe connu. À utiliser pour évaluer la capacité des modèles de langue N-grammes à bien estimer la vraisemblance d'un passage de texte.\n",
        "- *t2_proverbes_test2.json*: il contient des proverbes se terminant par des mots masqués. À utiliser pour générer des mots avec les modèles N-grammes qui remplaceront les masques.\n",
        "\n",
        "Consignes:\n",
        "- Utilisez NLTK pour construire les modèles de langue.\n",
        "- Utilisez NLTK (par exemple la fonction *word_tokenize*) pour la tokenisation des textes.\n",
        "- N'oubliez pas de faire le rebourrage (*padding*) des proverbes avec des symboles de début et de fin.\n",
        "- Faites un lissage de Laplace des modèles.\n",
        "- Il n’est pas nécessaire d’ajouter un jeton de mot inconnu (<UNK>)\n",
        "- Ne pas modifier les fonctions *load_proverbs* et *load_tests*.\n",
        "- Utilisez la variable *models* pour conserver les modèles après entraînement.\n",
        "- Ne pas modifier la signature de les fonctions *train_models* et *correct_proverb*.\n",
        "- Des modifications aux signatures de fonctions entraîneront des pénalités dans la correction.\n",
        "- Vous pouvez ajouter des cellules au *notebook* et ajouter toutes les fonctions utilitaires que vous voulez."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3ec4af",
      "metadata": {
        "id": "8a3ec4af"
      },
      "source": [
        "## Section 1 - Lecture des fichiers de données (proverbes et tests)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10c717f0",
      "metadata": {
        "id": "10c717f0"
      },
      "source": [
        "On définit ici 2 fonctions pour lire le fichier de proverbes et les 2 fichiers de test (un pour chacun des sous-tâches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "e10ec998",
      "metadata": {
        "id": "e10ec998"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Ne pas modifier le chemin de ces 2 fichiers pour faciliter notre correction\n",
        "proverbs_fn = \"./data/t2_proverbes.txt\"\n",
        "test1_fn = './data/t2_proverbes_test1.json'  # tests d'estimation de vraisemblance\n",
        "test2_fn = './data/t2_proverbes_test2.json'  # tests de génération de texte\n",
        "\n",
        "\n",
        "def load_proverbs(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        raw_lines = f.readlines()\n",
        "    return [x.strip() for x in raw_lines]\n",
        "\n",
        "\n",
        "def load_tests(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as fp:\n",
        "        test_data = json.load(fp)\n",
        "    return test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "c9fbbdd5",
      "metadata": {
        "id": "c9fbbdd5"
      },
      "outputs": [],
      "source": [
        "proverbs = load_proverbs(proverbs_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c650761",
      "metadata": {
        "id": "3c650761"
      },
      "source": [
        "Quelques informations à propos des proberbes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "dcd0c9a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcd0c9a9",
        "outputId": "be2dfd10-3f33-4c70-a8dd-d367182c09b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de proverbes pour l'entraînement: 3108\n",
            "\n",
            "Un exemple de proverbe: affaire menée sans bruit se fait avec plus de fruit\n"
          ]
        }
      ],
      "source": [
        "print(\"Nombre de proverbes pour l'entraînement: {}\".format(len(proverbs)))\n",
        "print(\"\\nUn exemple de proverbe: \" + proverbs[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b12b83",
      "metadata": {
        "id": "70b12b83"
      },
      "source": [
        "On monte également les 2 fichiers de test pour visualiser leur contenu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ee39d050",
      "metadata": {
        "id": "ee39d050"
      },
      "outputs": [],
      "source": [
        "tests_estimation = load_tests(test1_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "e80bd88a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "id": "e80bd88a",
        "outputId": "8e699325-1c7c-42d8-fca7-a2d22fef4073",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Candidates</th>\n",
              "      <th>Proverb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[a beau mentir qui vient de loin, a beau partir qui vient de loin, a beau dormir qui vient de loin]</td>\n",
              "      <td>a beau mentir qui vient de loin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[l’occasion fait le larron, l’occasion forge le bon homme, l'argent fait le larron]</td>\n",
              "      <td>l’occasion fait le larron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[pas de soucis, le ciel t’aidera, endors-toi, le ciel t’aidera, aide-toi, le ciel t’aidera, bouge, le ciel est bleu ...</td>\n",
              "      <td>aide-toi, le ciel t’aidera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[aide-toi, le ciel t’aura, aide-toi, le ciel t’aide, aide-toi, le ciel t’aidera]</td>\n",
              "      <td>aide-toi, le ciel t’aidera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ce que femme dit, dieu dit, ce que femme veut, dieu le veut, ce que femme veut, dieu aide et prête la main]</td>\n",
              "      <td>ce que femme veut, dieu le veut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[bien mal acquis jamais, bien mal acquis ne sait et ne profite jamais, bien mal acquis ne profite jamais]</td>\n",
              "      <td>bien mal acquis ne profite jamais</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[bon ouvrier ne travaille pas sans ses outils, bon ouvrier ne déplace pas ses outils, bon ouvrier ne querelle pas se...</td>\n",
              "      <td>bon ouvrier ne querelle pas ses outils</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[pour le fou, c’était tous les jours fête, pour le fou, ce sera tous les jours fête, pour le fou, c’est tous les jou...</td>\n",
              "      <td>pour le fou, c’est tous les jours fête</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[dire et dire, sont un, dire et faire, sont deux, dire, faire et plaire, sont trois]</td>\n",
              "      <td>dire et faire, sont deux</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[mieux vaut prévenir que guérir, mieux vaut prévenir, mieux vaut prévenir que courir et guérir]</td>\n",
              "      <td>mieux vaut prévenir que guérir</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>[mieux vaut dormir que guérir, mieux vaut prévenir que guérir, mieux vaut guérir que guérir]</td>\n",
              "      <td>mieux vaut prévenir que guérir</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[à qui dieu aide, nul ne peut nuire, à qui dieu parle souvent, nul ne peut entendre, à qui dieu nuit, nul ne peut ai...</td>\n",
              "      <td>à qui dieu aide, nul ne peut nuire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>[il faut le voir pour le croire, il faut le croire, il faut le boire pour le croire]</td>\n",
              "      <td>il faut le voir pour le croire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>[on ne vend pas le poisson qui est encore dans la mer, on ne mord pas le poisson qui est encore dans la mer, on ne v...</td>\n",
              "      <td>on ne vend pas le poisson qui est encore dans la mer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>[à vaincre sans gloire, n'est pas le bon début du proverbe, à vaincre sans péril, on triomphe sans gloire, On triomp...</td>\n",
              "      <td>à vaincre sans péril, on triomphe sans gloire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>[le poisson pourrit par la tête, le poisson respire et mange par la tête, le poisson a une drôle de tête]</td>\n",
              "      <td>le poisson pourrit par la tête</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>[couche plutôt sans souper, que de te lever avec des dettes, repose-toi plutôt sans souper, que de te lever avec des...</td>\n",
              "      <td>couche-toi plutôt sans souper, que de te lever avec des dettes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>[à rome, comme à rome, à rome, comme à québec, à québec, comme à rome]</td>\n",
              "      <td>à rome, comme à rome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>[quand le chat n’est pas là, les souris souris souris dansent, quand quand  quand  le chat n’est pas là, les souris ...</td>\n",
              "      <td>quand le chat n’est pas là, les souris dansent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>[les montagnes ne se rencontrent point, mais les hommes se rencontrent, les montagnes ne se déplacent point, mais le...</td>\n",
              "      <td>les montagnes ne se rencontrent point, mais les hommes se rencontrent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>[parler peu, chasse beaucoup de maladies, manger peu, chasse beaucoup de maladies, étudier peu, chasse beaucoup de m...</td>\n",
              "      <td>manger peu, chasse beaucoup de maladies</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                 Candidates  \\\n",
              "0                       [a beau mentir qui vient de loin, a beau partir qui vient de loin, a beau dormir qui vient de loin]   \n",
              "1                                       [l’occasion fait le larron, l’occasion forge le bon homme, l'argent fait le larron]   \n",
              "2   [pas de soucis, le ciel t’aidera, endors-toi, le ciel t’aidera, aide-toi, le ciel t’aidera, bouge, le ciel est bleu ...   \n",
              "3                                          [aide-toi, le ciel t’aura, aide-toi, le ciel t’aide, aide-toi, le ciel t’aidera]   \n",
              "4              [ce que femme dit, dieu dit, ce que femme veut, dieu le veut, ce que femme veut, dieu aide et prête la main]   \n",
              "5                 [bien mal acquis jamais, bien mal acquis ne sait et ne profite jamais, bien mal acquis ne profite jamais]   \n",
              "6   [bon ouvrier ne travaille pas sans ses outils, bon ouvrier ne déplace pas ses outils, bon ouvrier ne querelle pas se...   \n",
              "7   [pour le fou, c’était tous les jours fête, pour le fou, ce sera tous les jours fête, pour le fou, c’est tous les jou...   \n",
              "8                                      [dire et dire, sont un, dire et faire, sont deux, dire, faire et plaire, sont trois]   \n",
              "9                           [mieux vaut prévenir que guérir, mieux vaut prévenir, mieux vaut prévenir que courir et guérir]   \n",
              "10                             [mieux vaut dormir que guérir, mieux vaut prévenir que guérir, mieux vaut guérir que guérir]   \n",
              "11  [à qui dieu aide, nul ne peut nuire, à qui dieu parle souvent, nul ne peut entendre, à qui dieu nuit, nul ne peut ai...   \n",
              "12                                     [il faut le voir pour le croire, il faut le croire, il faut le boire pour le croire]   \n",
              "13  [on ne vend pas le poisson qui est encore dans la mer, on ne mord pas le poisson qui est encore dans la mer, on ne v...   \n",
              "14  [à vaincre sans gloire, n'est pas le bon début du proverbe, à vaincre sans péril, on triomphe sans gloire, On triomp...   \n",
              "15                [le poisson pourrit par la tête, le poisson respire et mange par la tête, le poisson a une drôle de tête]   \n",
              "16  [couche plutôt sans souper, que de te lever avec des dettes, repose-toi plutôt sans souper, que de te lever avec des...   \n",
              "17                                                   [à rome, comme à rome, à rome, comme à québec, à québec, comme à rome]   \n",
              "18  [quand le chat n’est pas là, les souris souris souris dansent, quand quand  quand  le chat n’est pas là, les souris ...   \n",
              "19  [les montagnes ne se rencontrent point, mais les hommes se rencontrent, les montagnes ne se déplacent point, mais le...   \n",
              "20  [parler peu, chasse beaucoup de maladies, manger peu, chasse beaucoup de maladies, étudier peu, chasse beaucoup de m...   \n",
              "\n",
              "                                                                  Proverb  \n",
              "0                                         a beau mentir qui vient de loin  \n",
              "1                                               l’occasion fait le larron  \n",
              "2                                              aide-toi, le ciel t’aidera  \n",
              "3                                              aide-toi, le ciel t’aidera  \n",
              "4                                         ce que femme veut, dieu le veut  \n",
              "5                                       bien mal acquis ne profite jamais  \n",
              "6                                  bon ouvrier ne querelle pas ses outils  \n",
              "7                                  pour le fou, c’est tous les jours fête  \n",
              "8                                                dire et faire, sont deux  \n",
              "9                                          mieux vaut prévenir que guérir  \n",
              "10                                         mieux vaut prévenir que guérir  \n",
              "11                                     à qui dieu aide, nul ne peut nuire  \n",
              "12                                         il faut le voir pour le croire  \n",
              "13                   on ne vend pas le poisson qui est encore dans la mer  \n",
              "14                          à vaincre sans péril, on triomphe sans gloire  \n",
              "15                                         le poisson pourrit par la tête  \n",
              "16         couche-toi plutôt sans souper, que de te lever avec des dettes  \n",
              "17                                                   à rome, comme à rome  \n",
              "18                         quand le chat n’est pas là, les souris dansent  \n",
              "19  les montagnes ne se rencontrent point, mais les hommes se rencontrent  \n",
              "20                                manger peu, chasse beaucoup de maladies  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_dataframe(test_proverbs):\n",
        "    return pd.DataFrame.from_dict(test_proverbs, orient='columns', dtype=None, columns=None)\n",
        "\n",
        "df = get_dataframe(tests_estimation)\n",
        "pd.set_option(\"display.max_colwidth\", 120)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "568b91b3",
      "metadata": {
        "id": "568b91b3"
      },
      "outputs": [],
      "source": [
        "tests_generation = load_tests(test2_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "3e7a837e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "3e7a837e",
        "outputId": "0c46bcf4-dbd3-439e-8bb8-153b7e66282f",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Masked</th>\n",
              "      <th>Proverb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a beau mentir qui vient de ***</td>\n",
              "      <td>a beau mentir qui vient de loin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a beau mentir qui vient *** ***</td>\n",
              "      <td>a beau mentir qui vient de loin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a beau mentir qui *** *** ***</td>\n",
              "      <td>a beau mentir qui vient de loin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>l’occasion fait *** ***</td>\n",
              "      <td>l’occasion fait le larron</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aide-toi, le ciel t’***</td>\n",
              "      <td>aide-toi, le ciel t’aidera</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ce que femme veut, dieu le ***</td>\n",
              "      <td>ce que femme veut, dieu le veut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ce que femme veut, dieu *** ***</td>\n",
              "      <td>ce que femme veut, dieu le veut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bien mal acquis ne profite ***</td>\n",
              "      <td>bien mal acquis ne profite jamais</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bon ouvrier ne querelle pas ses ***</td>\n",
              "      <td>bon ouvrier ne querelle pas ses outils</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bon ouvrier ne querelle *** *** ***</td>\n",
              "      <td>bon ouvrier ne querelle pas ses outils</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>pour le fou, c’est tous les jours ***</td>\n",
              "      <td>pour le fou, c’est tous les jours fête</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>dire et faire, sont ***</td>\n",
              "      <td>dire et faire, sont deux</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>dire et faire, *** ***</td>\n",
              "      <td>dire et faire, sont deux</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>mieux vaut prévenir que ***</td>\n",
              "      <td>mieux vaut prévenir que guérir</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>mieux vaut prévenir *** ***</td>\n",
              "      <td>mieux vaut prévenir que guérir</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>à qui dieu aide, nul ne peut ***</td>\n",
              "      <td>à qui dieu aide, nul ne peut nuire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>à qui dieu aide, nul ne *** ***</td>\n",
              "      <td>à qui dieu aide, nul ne peut nuire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>il faut le voir pour le ***</td>\n",
              "      <td>il faut le voir pour le croire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>il faut le voir *** *** ***</td>\n",
              "      <td>il faut le voir pour le croire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>on ne vend pas le poisson qui est encore dans la ***</td>\n",
              "      <td>on ne vend pas le poisson qui est encore dans la mer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>on ne vend pas le poisson qui est encore *** *** ***</td>\n",
              "      <td>on ne vend pas le poisson qui est encore dans la mer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>le poisson pourrit par la ***</td>\n",
              "      <td>le poisson pourrit par la tête</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>couche-toi plutôt sans souper, que de te lever avec des ***</td>\n",
              "      <td>couche-toi plutôt sans souper, que de te lever avec des dettes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>couche-toi plutôt sans souper, que de te lever *** *** ***</td>\n",
              "      <td>couche-toi plutôt sans souper, que de te lever avec des dettes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>les montagnes ne se rencontrent point, mais les hommes se ***</td>\n",
              "      <td>les montagnes ne se rencontrent point, mais les hommes se rencontrent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>les montagnes ne se rencontrent point, mais les hommes *** ***</td>\n",
              "      <td>les montagnes ne se rencontrent point, mais les hommes se rencontrent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>manger peu, chasse beaucoup *** ***</td>\n",
              "      <td>manger peu, chasse beaucoup de maladies</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                            Masked  \\\n",
              "0                                   a beau mentir qui vient de ***   \n",
              "1                                  a beau mentir qui vient *** ***   \n",
              "2                                    a beau mentir qui *** *** ***   \n",
              "3                                          l’occasion fait *** ***   \n",
              "4                                          aide-toi, le ciel t’***   \n",
              "5                                   ce que femme veut, dieu le ***   \n",
              "6                                  ce que femme veut, dieu *** ***   \n",
              "7                                   bien mal acquis ne profite ***   \n",
              "8                              bon ouvrier ne querelle pas ses ***   \n",
              "9                              bon ouvrier ne querelle *** *** ***   \n",
              "10                           pour le fou, c’est tous les jours ***   \n",
              "11                                         dire et faire, sont ***   \n",
              "12                                          dire et faire, *** ***   \n",
              "13                                     mieux vaut prévenir que ***   \n",
              "14                                     mieux vaut prévenir *** ***   \n",
              "15                                à qui dieu aide, nul ne peut ***   \n",
              "16                                 à qui dieu aide, nul ne *** ***   \n",
              "17                                     il faut le voir pour le ***   \n",
              "18                                     il faut le voir *** *** ***   \n",
              "19            on ne vend pas le poisson qui est encore dans la ***   \n",
              "20            on ne vend pas le poisson qui est encore *** *** ***   \n",
              "21                                   le poisson pourrit par la ***   \n",
              "22     couche-toi plutôt sans souper, que de te lever avec des ***   \n",
              "23      couche-toi plutôt sans souper, que de te lever *** *** ***   \n",
              "24   les montagnes ne se rencontrent point, mais les hommes se ***   \n",
              "25  les montagnes ne se rencontrent point, mais les hommes *** ***   \n",
              "26                             manger peu, chasse beaucoup *** ***   \n",
              "\n",
              "                                                                  Proverb  \n",
              "0                                         a beau mentir qui vient de loin  \n",
              "1                                         a beau mentir qui vient de loin  \n",
              "2                                         a beau mentir qui vient de loin  \n",
              "3                                               l’occasion fait le larron  \n",
              "4                                              aide-toi, le ciel t’aidera  \n",
              "5                                         ce que femme veut, dieu le veut  \n",
              "6                                         ce que femme veut, dieu le veut  \n",
              "7                                       bien mal acquis ne profite jamais  \n",
              "8                                  bon ouvrier ne querelle pas ses outils  \n",
              "9                                  bon ouvrier ne querelle pas ses outils  \n",
              "10                                 pour le fou, c’est tous les jours fête  \n",
              "11                                               dire et faire, sont deux  \n",
              "12                                               dire et faire, sont deux  \n",
              "13                                         mieux vaut prévenir que guérir  \n",
              "14                                         mieux vaut prévenir que guérir  \n",
              "15                                     à qui dieu aide, nul ne peut nuire  \n",
              "16                                     à qui dieu aide, nul ne peut nuire  \n",
              "17                                         il faut le voir pour le croire  \n",
              "18                                         il faut le voir pour le croire  \n",
              "19                   on ne vend pas le poisson qui est encore dans la mer  \n",
              "20                   on ne vend pas le poisson qui est encore dans la mer  \n",
              "21                                         le poisson pourrit par la tête  \n",
              "22         couche-toi plutôt sans souper, que de te lever avec des dettes  \n",
              "23         couche-toi plutôt sans souper, que de te lever avec des dettes  \n",
              "24  les montagnes ne se rencontrent point, mais les hommes se rencontrent  \n",
              "25  les montagnes ne se rencontrent point, mais les hommes se rencontrent  \n",
              "26                                manger peu, chasse beaucoup de maladies  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = get_dataframe(tests_generation)\n",
        "pd.set_option(\"display.max_colwidth\", 100)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c57364c",
      "metadata": {
        "id": "3c57364c"
      },
      "source": [
        "## Section 2 - Construction des modèles de langue N-grammes.\n",
        "\n",
        "La fonction ***train_models*** prend en entrée une liste de proverbes et construit les trois modèles unigramme, bigramme et trigramme.\n",
        "\n",
        "Les 3 modèles entraînés sont conservés dans ***models***, un dictionnaire Python qui prend la forme\n",
        "\n",
        "<pre>\n",
        "{\n",
        "   1: modele_unigramme,\n",
        "   2: modele_bigramme,\n",
        "   3: modele_trigramme\n",
        "}\n",
        "</pre>\n",
        "\n",
        "avec comme clé la valeur N du modèle et comme valeur le modèle construit par NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "933cb663",
      "metadata": {
        "id": "933cb663"
      },
      "source": [
        "Metter dans les prochaines cellules le code pour construire vos modèles avec NLTK, pour obtenir les n-grammes de mots, pour déterminer le vocabulaire, etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "08e62b25",
      "metadata": {
        "id": "08e62b25"
      },
      "outputs": [],
      "source": [
        "# Import des modules nécessaires\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams, pad_sequence\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm.models import Laplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "5d4c6390",
      "metadata": {
        "id": "5d4c6390"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "BOS = '<BOS>'  # Jeton de début de proverbe\n",
        "EOS = '<EOS>'  # Jeton de fin de proverbe\n",
        "\n",
        "models = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "9bb1ec46",
      "metadata": {
        "id": "9bb1ec46"
      },
      "outputs": [],
      "source": [
        "# Fonction pour construire le vocabulaire\n",
        "\n",
        "def build_vocabulary(proverbs):\n",
        "    \"\"\"\n",
        "    Construit le vocabulaire (liste des mots distincts) à partir des proverbes.\n",
        "    On ajoute BOS et EOS explicitement pour être sûrs qu'ils sont inclus.\n",
        "    \"\"\"\n",
        "    all_tokens = []\n",
        "    for sentence in proverbs:\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "        all_tokens.extend(tokens)\n",
        "    voc = set(all_tokens)\n",
        "    voc.add(BOS)\n",
        "    voc.add(EOS)\n",
        "    return list(voc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "6e460425",
      "metadata": {
        "id": "6e460425"
      },
      "outputs": [],
      "source": [
        "# Fonction pour obtenir les n-grammes avec padding\n",
        "\n",
        "def get_ngrams(proverbs, n=2):\n",
        "    \"\"\"\n",
        "    Retourne tous les n-grammes (avec padding) pour un corpus de proverbes.\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "    for sentence in proverbs:\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "        padded_sent = list(\n",
        "            pad_sequence(tokens,\n",
        "                         pad_left=True, left_pad_symbol=BOS,\n",
        "                         pad_right=True, right_pad_symbol=EOS,\n",
        "                         n=n)\n",
        "        )\n",
        "        all_ngrams.extend(list(ngrams(padded_sent, n=n)))\n",
        "    return all_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "6c9a6991",
      "metadata": {
        "id": "6c9a6991"
      },
      "outputs": [],
      "source": [
        "# Fonction utilitaire pour pipeline NLTK (on met les données dans la format attendu par NLTK, notamment padding)\n",
        "\n",
        "def prepare_data(proverbs, n):\n",
        "    \"\"\"\n",
        "    Utilise la fonction padded_everygram_pipeline de NLTK\n",
        "    pour générer les n-grammes et le vocabulaire associés.\n",
        "    \"\"\"\n",
        "    tokenized = [word_tokenize(p.lower()) for p in proverbs]\n",
        "    train_data, vocab = padded_everygram_pipeline(n, tokenized)\n",
        "    return train_data, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "03383121",
      "metadata": {
        "id": "03383121"
      },
      "outputs": [],
      "source": [
        "#Fonction principale pour entraîner les modèles\n",
        "def train_models(proverbs):\n",
        "    \"\"\" Vous ajoutez à partir d'ici le code dont vous avez besoin\n",
        "        pour construire les différents modèles N-grammes.\n",
        "        Cette fonction doit construire tous les modèles en une seule passe.\n",
        "        Voir les consignes de l'énoncé du travail pratique concernant les modèles à entraîner.\n",
        "\n",
        "        Vous pouvez ajouter les fonctions/méthodes et variables que vous jugerez nécessaire.\n",
        "        Merci de ne pas modifier la signature et le comportement de cette fonction (nom, arguments, sauvegarde des modèles).\n",
        "    \"\"\"\n",
        "\n",
        "    for n in [1, 2, 3]:\n",
        "        # Prépare les données (ngrams + vocabulaire + padding) pour cet ordre\n",
        "        train_data, vocab = prepare_data(proverbs, n)\n",
        "\n",
        "        # Construis le modèle avec Laplace smoothing\n",
        "        model = Laplace(n)\n",
        "\n",
        "        # Entraîne le modèle\n",
        "        model.fit(train_data, vocab)\n",
        "\n",
        "        # Sauvegarde dans le dictionnaire global\n",
        "        models[n] = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "dd160430",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd160430",
        "outputId": "5f077f2f-b7f0-433e-aa3a-365e7b2ca53b"
      },
      "outputs": [],
      "source": [
        "# Entraînement des 3 modèles\n",
        "train_models(proverbs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "b755d014",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b755d014",
        "outputId": "137da0f1-9988-4a28-b76d-830d40fede7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clés disponibles dans 'models': dict_keys([1, 2, 3])\n",
            "\n",
            "[Unigramme] P('mieux') = 0.003018336393591066\n",
            "\n",
            "[Bigramme] P('vaut' | 'mieux') = 0.011326508385972555\n",
            "\n",
            "[Trigramme] P('prévenir' | 'mieux vaut') = 0.00044218439089100157\n",
            "\n",
            "[Bigramme] P('xyz' | 'mieux') = 0.00021781746896101068\n",
            "\n",
            "[Bigramme] Perplexité sur 'mieux vaut prévenir que guérir' = 838.1694486228748\n"
          ]
        }
      ],
      "source": [
        "# 1. On vérifie qu’on a bien les trois modèles\n",
        "print(\"Clés disponibles dans 'models':\", models.keys())\n",
        "\n",
        "# 2. Exemple : probabilité d’un mot avec unigramme\n",
        "print(\"\\n[Unigramme] P('mieux') =\", models[1].score('mieux'))\n",
        "\n",
        "# 3. Exemple : probabilité conditionnelle avec bigramme\n",
        "print(\"\\n[Bigramme] P('vaut' | 'mieux') =\", models[2].score('vaut', ['mieux']))\n",
        "\n",
        "# 4. Exemple : probabilité conditionnelle avec trigramme\n",
        "print(\"\\n[Trigramme] P('prévenir' | 'mieux vaut') =\", models[3].score('prévenir', ['mieux','vaut']))\n",
        "\n",
        "# 5. Vérifier qu’un mot inconnu n’a pas probabilité 0 (Laplace smoothing)\n",
        "print(\"\\n[Bigramme] P('xyz' | 'mieux') =\", models[2].score('xyz', ['mieux']))\n",
        "\n",
        "# 6. Vérifier perplexité sur une courte séquence connue\n",
        "test_seq = list(ngrams(['mieux','vaut','prévenir','que','guérir'], n=2))\n",
        "print(\"\\n[Bigramme] Perplexité sur 'mieux vaut prévenir que guérir' =\", models[2].perplexity(test_seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a15bee",
      "metadata": {
        "id": "f2a15bee"
      },
      "source": [
        "Les vérifications montrent que :\n",
        "- Les trois modèles (unigramme, bigramme, trigramme) ont bien été construits et stockés dans models.\n",
        "- Les probabilités estimées sont cohérentes :\n",
        "    Unigrammes → fréquence brute des mots dans le corpus.\n",
        "    Bigrammes / Trigrammes → probabilités conditionnelles qui diminuent quand le contexte est plus long.\n",
        "- Le lissage de Laplace fonctionne : même un mot totalement inconnu (xyz) reçoit une probabilité non nulle.\n",
        "- La perplexité calculée sur un proverbe connu est élevée (≈ 838). Cela reflète le fait que le corpus est vaste et varié : le modèle a du mal à être très confiant sur une courte séquence. On s’attend à ce que la perplexité diminue quand on utilisera les trigrammes, qui capturent plus de contexte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a05719e",
      "metadata": {
        "id": "0a05719e"
      },
      "source": [
        "## Section 3 - Choisir le proverbe connu parmi plusieurs candidats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40f8c38",
      "metadata": {
        "id": "d40f8c38"
      },
      "source": [
        "Expliquez ici comment vous procédez pour choisir une option parmi les candidats proposés."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb852772",
      "metadata": {
        "id": "fb852772"
      },
      "source": [
        "Pour chaque question du fichier t2_proverbes_test1.json, on dispose :\n",
        "- d’une liste de phrases candidates,\n",
        "- d’un proverbe correct.\n",
        "\n",
        "L’objectif est de sélectionner, à l’aide de nos modèles N-grammes (unigramme, bigramme, trigramme), le candidat qui a la probabilité la plus élevée selon le modèle.\n",
        "\n",
        "Notre démarche est la suivante :\n",
        "\n",
        "1 - Tokenisation et padding :\n",
        "- Chaque candidat est découpé en mots avec word_tokenize.\n",
        "- On ajoute les symboles `<BOS>` et `<EOS>` selon l’ordre du modèle (1, 2 ou 3).\n",
        "\n",
        "2 - Calcul de la vraisemblance\n",
        "- Pour un candidat donné, on décompose sa phrase en n-grammes.\n",
        "- On multiplie les probabilités des n-grammes successifs fournies par le modèle.\n",
        "\n",
        "3 - Choix du meilleur candidat\n",
        "- On compare les scores obtenus pour tous les candidats.\n",
        "- Celui qui minimise la perplexité (ou maximise la probabilité) est retenu comme le proverbe prédit par le modèle.\n",
        "\n",
        "4 - Évaluation des performances\n",
        "- On compare la prédiction avec le proverbe correct donné dans le fichier de test.\n",
        "- On calcule le taux de succès pour chaque modèle (unigramme, bigramme, trigramme) :\n",
        "    - La métrique qu’on utilise est l’accuracy\n",
        "    - Elle mesure simplement la proportion de fois où le modèle choisit le bon proverbe parmi les candidats.\n",
        "    - cela suffit dans notre cas car chaque test a exactement une seule bonne réponse, il n’y a pas de classes déséquilibrées (chaque test est binaire : correct / incorrect), pas besoin de métriques plus fines comme précision, rappel ou F1 (utiles surtout quand il y a plusieurs classes ou un déséquilibre)\n",
        "\n",
        "=======================================================================================================================================\n",
        "\n",
        "On imagine déjà que :\n",
        "- Les unigrammes se basent uniquement sur la fréquence brute des mots, sans tenir compte de l’ordre → donc peu précis.\n",
        "- Les bigrammes ajoutent une dépendance au mot précédent, ce qui améliore la cohérence des proverbes.\n",
        "- Les trigrammes capturent encore plus de contexte et devraient mieux discriminer les candidats.\n",
        "\n",
        "Avec cette méthode, on peut analyser l’impact de la longueur de l’historique (N=1,2,3) sur la capacité des modèles à reconnaître les proverbes corrects.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36bba545",
      "metadata": {
        "id": "36bba545"
      },
      "source": [
        "Voici les tests à utiliser pour cette sous-tâche. Chaque test contient une liste de candidats et le proverbe connu qui devrait être choisi par le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "ddd89024",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd89024",
        "outputId": "0b46a96f-84db-4d47-c610-82e7ab87072f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de tests: 21\n"
          ]
        }
      ],
      "source": [
        "print(\"Nombre de tests:\", len(tests_estimation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "3c761cd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c761cd1",
        "outputId": "ab737632-f786-44c8-8bc9-9d0b2e165e0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Candidates': ['a beau mentir qui vient de loin',\n",
              "  'a beau partir qui vient de loin',\n",
              "  'a beau dormir qui vient de loin'],\n",
              " 'Proverb': 'a beau mentir qui vient de loin'}"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tests_estimation[0]  # Un exemple de test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3607604",
      "metadata": {
        "id": "e3607604"
      },
      "source": [
        "Complétez la fonction suivante qui fait le choix de l'option qui semble la plus vraisemblable parmi une liste de candidats. Ne pas modifier la signature de la fonction ni ses valeurs de retour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "fdf0b1b1",
      "metadata": {
        "id": "fdf0b1b1"
      },
      "outputs": [],
      "source": [
        "# Fonction d'évaluation\n",
        "\n",
        "def select_proverb(candidates, n=3):\n",
        "    \"\"\"Retourne le candidat retenu et son score basé sur la perplexité.\n",
        "\n",
        "    *candidates* : liste de textes candidats (dont un seul est correct)\n",
        "    *n* : ordre du modèle (1 = unigramme, 2 = bigramme, 3 = trigramme)\n",
        "    \"\"\"\n",
        "\n",
        "    model = models[n]\n",
        "    best_score = float('inf')  # on cherche la plus faible perplexité\n",
        "    result = None\n",
        "\n",
        "    for cand in candidates:\n",
        "        # Extraire les n-grammes du candidat\n",
        "        ngrams_cand = get_ngrams([cand], n=n)\n",
        "\n",
        "        # Calculer la perplexité sur la séquence\n",
        "        try:\n",
        "            score = model.perplexity(ngrams_cand)\n",
        "        except ZeroDivisionError:\n",
        "            # Cas pathologique si la phrase est très courte ou OOV\n",
        "            score = float('inf')\n",
        "\n",
        "        # On choisit le candidat avec la perplexité la plus faible\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            result = cand\n",
        "\n",
        "    return result, best_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727b93a2",
      "metadata": {
        "id": "727b93a2"
      },
      "source": [
        "**REMARQUE**\n",
        "\n",
        "Initialement, on avait envisagé de faire la sélection du proverbe en sommant les log-probabilités des n-grammes, mais cette méthode favorisait artificiellement les proverbes courts (moins de termes négatifs à additionner) : `score = sum(model.logscore(ng[-1], list(ng[:-1])) for ng in ngrams_cand)`\n",
        "\n",
        "Nous avons amélioré la fonction en utilisant la perplexité comme critère de choix : c’est une mesure normalisée par la longueur et standard en modélisation de langue, qui permet de comparer équitablement les candidats."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbf4bc8",
      "metadata": {
        "id": "1cbf4bc8"
      },
      "source": [
        "Mettez dans les prochaines cellules le code nécessaire afin d'évaluer chacun des modèles pour cette sous-tâche."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "ef8b313c",
      "metadata": {
        "id": "ef8b313c"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(tests, n=3):\n",
        "    \"\"\"\n",
        "    Évalue un modèle N-gramme (n = 1, 2 ou 3) sur la sous-tâche 1.\n",
        "    Retourne le nombre de bonnes réponses, le total et l'accuracy.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = len(tests)\n",
        "\n",
        "    for test in tests:\n",
        "        candidates = test[\"Candidates\"]\n",
        "        gold = test[\"Proverb\"]\n",
        "\n",
        "        predicted, score = select_proverb(candidates, n=n)\n",
        "\n",
        "        if predicted == gold:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return correct, total, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "4b4bac82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b4bac82",
        "outputId": "073782ba-1d97-4074-aef6-105fa14a65aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle 1-grammes : 8/21 corrects  (Accuracy = 38.10%)\n"
          ]
        }
      ],
      "source": [
        "# Évaluation modèle unigramme (N=1)\n",
        "\n",
        "correct, total, acc = evaluate_model(tests_estimation, n=1)\n",
        "print(f\"Modèle 1-grammes : {correct}/{total} corrects  (Accuracy = {acc:.2%})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "fb0f79b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb0f79b7",
        "outputId": "2ef5d702-6cc8-4412-a885-e79149ab021f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle 2-grammes : 15/21 corrects  (Accuracy = 71.43%)\n"
          ]
        }
      ],
      "source": [
        "# Évaluation modèle bigramme (N=2)\n",
        "\n",
        "correct, total, acc = evaluate_model(tests_estimation, n=2)\n",
        "print(f\"Modèle 2-grammes : {correct}/{total} corrects  (Accuracy = {acc:.2%})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "ca71b1e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca71b1e7",
        "outputId": "5b3bdb11-76cd-4742-ba6d-d2c505f09529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle 3-grammes : 20/21 corrects  (Accuracy = 95.24%)\n"
          ]
        }
      ],
      "source": [
        "# Évaluation modèle trigramme (N=3)\n",
        "\n",
        "correct, total, acc = evaluate_model(tests_estimation, n=3)\n",
        "print(f\"Modèle 3-grammes : {correct}/{total} corrects  (Accuracy = {acc:.2%})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e4932e",
      "metadata": {
        "id": "71e4932e"
      },
      "source": [
        "Présentez ici vos résultats et répondez aux questions formulées dans l'énoncé du travail. Vous pouvez ajouter des cellules au besoin."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ab61d3",
      "metadata": {
        "id": "03ab61d3"
      },
      "source": [
        "### Résultats obtenus\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c9080a",
      "metadata": {
        "id": "24c9080a"
      },
      "source": [
        "Unigramme : 8/21 corrects → 38.1%\n",
        "\n",
        "Bigramme : 15/21 corrects → 71.4%\n",
        "\n",
        "Trigramme : 20/21 corrects → 95.2%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971f698c",
      "metadata": {
        "id": "971f698c"
      },
      "source": [
        "### Analyse\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9004e1c5",
      "metadata": {
        "id": "9004e1c5"
      },
      "source": [
        "La sélection du proverbe se fait en comparant la perplexité des candidats : celui avec la plus faible perplexité est choisi.\n",
        "\n",
        "Les performances montrent un écart clair entre les modèles :\n",
        "- L’unigramme est trop limité : il ne considère que les fréquences individuelles des mots, sans ordre → faible précision.\n",
        "- Le bigramme capte des associations locales fréquentes et améliore nettement la reconnaissance.\n",
        "- Le trigramme exploite un contexte plus riche et capture beaucoup mieux la structure proverbiale → précision quasi parfaite.\n",
        "\n",
        "\n",
        "### Impact de la longueur de l'historique (N)\n",
        "\n",
        "Donc comme on l'imaginait, plus la longueur de l’historique augmente, plus le modèle reflète fidèlement le langage des proverbes.\n",
        "\n",
        "**Explications :**\n",
        "- **N=1 (Unigramme)** : Considère uniquement les fréquences individuelles des mots,\n",
        "  sans tenir compte de leur ordre → incapable de distinguer une structure proverbiale\n",
        "  d'une suite aléatoire de mots courants.\n",
        "  \n",
        "- **N=2 (Bigramme)** : Capture des associations locales fréquentes → amélioration nette de la reconnaissance des structures typiques.\n",
        "  \n",
        "- **N=3 (Trigramme)** : Exploite un contexte plus riche et reconnaît des expressions\n",
        "  figées complètes → précision quasi parfaite.\n",
        "\n",
        "**Conclusion :** Plus la longueur de l'historique augmente, plus le modèle reflète\n",
        "fidèlement le langage des proverbes. Le contexte supplémentaire permet de capturer\n",
        "les structures caractéristiques.\n",
        "\n",
        "### Capacité à capturer le langage des proverbes\n",
        "\n",
        "- **Unigramme** : Capture mal le langage des proverbes (seulement le vocabulaire,\n",
        "  pas la structure)\n",
        "  \n",
        "- **Bigramme** : Capture partiellement (reconnaît des paires de mots typiques mais\n",
        "  manque les expressions plus longues)\n",
        "  \n",
        "- **Trigramme** : Capture très bien le langage des proverbes (reconnaît les\n",
        "  expressions figées et structures idiomatiques)\n",
        "\n",
        "Le trigramme reproduit fidèlement les régularités lexicales et syntaxiques des proverbes.\n",
        "Sa réussite à 95% démontre qu'un contexte de 3 mots suffit à identifier la plupart des\n",
        "patterns caractéristiques. L'unique erreur suggère que certains proverbes rares ou\n",
        "ambigus restent difficiles à distinguer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49977ac",
      "metadata": {
        "id": "b49977ac"
      },
      "source": [
        "## Section 4 - Remplacer les masques du proverbe avec des mots générés"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0bdcaf",
      "metadata": {
        "id": "2b0bdcaf"
      },
      "source": [
        "Utilisez pour cette sous-tâche les proverbes masqués (dans la variable *tests_generation*) obtenus à la Section 1 à partir du fichier *'./data/t2_proverbes_test2.json'*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "d327bc59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d327bc59",
        "outputId": "db728525-90d4-4174-b8dd-8c2fada611e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tests_generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "ba81dc20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba81dc20",
        "outputId": "880cd9ac-8591-407a-b81b-0821e200d537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'Masked': 'a beau mentir qui vient de ***',\n",
              "  'Proverb': 'a beau mentir qui vient de loin'},\n",
              " {'Masked': 'a beau mentir qui vient *** ***',\n",
              "  'Proverb': 'a beau mentir qui vient de loin'},\n",
              " {'Masked': 'a beau mentir qui *** *** ***',\n",
              "  'Proverb': 'a beau mentir qui vient de loin'},\n",
              " {'Masked': 'l’occasion fait *** ***', 'Proverb': 'l’occasion fait le larron'},\n",
              " {'Masked': 'aide-toi, le ciel t’***',\n",
              "  'Proverb': 'aide-toi, le ciel t’aidera'}]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tests_generation[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "647fac2c",
      "metadata": {
        "id": "647fac2c"
      },
      "source": [
        "Compléter le code dans les prochaines cellules pour remplacer les masques par des mots générés. Vous pouvez ajouter des cellules au besoin."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e82623a",
      "metadata": {
        "id": "8e82623a"
      },
      "source": [
        "### Génération de proverbes masqués avec modèles N-grammes (Laplace, N=1..3)\n",
        "\n",
        "Nous utilisons des modèles de langue **N-grammes** (avec lissage **Laplace**) comme **générateurs** pour compléter des proverbes contenant des masques.\n",
        "\n",
        "#### Idée générale\n",
        "Étant donné un proverbe masqué (ex. *« a beau mentir qui vient de *** »*), on **remplit les masques `***`** en générant des mots probables selon le **contexte gauche** et, après coup, on **re-classe** les propositions en fonction de la **phrase entière**.\n",
        "\n",
        "#### Procédure\n",
        "- **Tokénisation NLTK** et **minuscules** (le corpus d’entraînement est en minuscules).\n",
        "- **Génération mot par mot** ; pour les **runs de masques consécutifs** (ex. `*** *** ***`), on applique un **beam search court** (top-k/beam) pour explorer quelques suites plausibles.\n",
        "- On **bloque `<EOS>`** tant qu’il reste des masques à compléter (évite de couper trop tôt).\n",
        "- On **favorise un “mot de contenu”** pour **terminer un run** (évite de finir par *de/le/la*).\n",
        "- On réalise ensuite un **re-ranking par perplexité** de la **phrase complète** (BOS/EOS + *everygrams* NLTK) afin de retenir la séquence qui rend **toute** la phrase la plus naturelle selon le modèle.\n",
        "\n",
        "#### Évaluation\n",
        "- **Accuracy “phrase”** : exact match entre la prédiction et le gold **après normalisation** (apostrophes/élisions).\n",
        "- **Accuracy “mots masqués”** (demandée explicitement) : proportion de **mots générés** qui correspondent **aux positions masquées** du proverbe de référence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "c82750c9",
      "metadata": {
        "id": "c82750c9"
      },
      "outputs": [],
      "source": [
        "# Fonction de normalisation & comparaison par tokens\n",
        "def tokens_norm(s: str):\n",
        "    \"\"\"\n",
        "    Normalise une chaîne pour la comparaison token-par-token.\n",
        "\n",
        "    Choix assumé : on *casse* les élisions en remplaçant toute apostrophe par un espace.\n",
        "    - mise en minuscules ;\n",
        "    - remplacement des apostrophes typographiques (’ U+2019) et ASCII (') par un espace ;\n",
        "    - réduction des espaces multiples ;\n",
        "    - tokenisation NLTK.\n",
        "    \"\"\"\n",
        "    # minuscules\n",
        "    s = s.lower()\n",
        "    # remplacer toutes formes d'apostrophe par un espace (on casse les élisions)\n",
        "    s = s.replace(\"’\", \" \").replace(\"'\", \" \")\n",
        "    # normaliser les blancs\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    # tokeniser\n",
        "    toks = word_tokenize(s)\n",
        "    return toks\n",
        "\n",
        "def eq_by_tokens(pred_str, gold_str):\n",
        "    \"\"\"\n",
        "    Compare deux phrases à l'égalité stricte de leurs séquences de tokens normalisés\n",
        "    (avec élisions cassées).\n",
        "    \"\"\"\n",
        "    return tokens_norm(pred_str) == tokens_norm(gold_str)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "35fc8118",
      "metadata": {
        "id": "35fc8118"
      },
      "outputs": [],
      "source": [
        "# Utilitaires de génération & perplexité\n",
        "import re\n",
        "from nltk.util import pad_sequence, ngrams as _ngrams\n",
        "\n",
        "def _is_word_like(w: str) -> bool:\n",
        "    \"\"\"Mot 'alphabétique' FR (lettres accentuées et tirets intra-mot).\"\"\"\n",
        "    return re.fullmatch(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+(?:-[A-Za-zÀ-ÖØ-öø-ÿ]+)*\", w) is not None\n",
        "\n",
        "def _left_context_with_bos(generated, order):\n",
        "    \"\"\"Derniers n-1 mots ; si insuffisant (début de phrase), pad avec <BOS>.\"\"\"\n",
        "    if order <= 1:\n",
        "        return []\n",
        "    need = order - 1\n",
        "    ctx = generated[-need:]\n",
        "    if len(ctx) < need:\n",
        "        ctx = [\"<BOS>\"] * (need - len(ctx)) + ctx\n",
        "    return ctx\n",
        "\n",
        "def sent_ngrams_from_tokens(tokens, n):\n",
        "    \"\"\"BOS/EOS + n-grammes de la phrase tokenisée.\"\"\"\n",
        "    padded = list(pad_sequence(tokens, pad_left=True, left_pad_symbol=\"<BOS>\",\n",
        "                               pad_right=True, right_pad_symbol=\"<EOS>\", n=n))\n",
        "    return list(_ngrams(padded, n=n))\n",
        "\n",
        "def full_sentence_perplexity(model, tokens):\n",
        "    \"\"\"Perplexité de la phrase complète vue par le modèle (ordre = model.order).\"\"\"\n",
        "    seq = sent_ngrams_from_tokens(tokens, model.order)\n",
        "    return model.perplexity(seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "fb9ad4cb",
      "metadata": {
        "id": "fb9ad4cb"
      },
      "outputs": [],
      "source": [
        "# Stoplist: on évite de finir un run par un mot-outil très court\n",
        "STOP_LAST = {\n",
        "    \"de\",\"du\",\"des\",\"le\",\"la\",\"les\",\"un\",\"une\",\"au\",\"aux\",\"l\",\"d\",\n",
        "    \"et\",\"ou\",\"à\",\"en\",\"que\",\"qui\",\"ne\",\"pas\",\"pour\",\"dans\",\"se\",\n",
        "    \"ce\",\"ces\",\"leur\",\"leurs\",\"sur\",\"son\",\"sa\",\"ses\",\"y\",\"on\",\"a\",\n",
        "    \"est\",\"sont\",\"tu\",\"il\",\"elle\",\"ils\",\"elles\",\"je\",\"me\",\"te\",\"se\",\n",
        "    \"plus\"\n",
        "}\n",
        "\n",
        "def top_k_next_words(model, context, k=8, allow_eos=True, forbid_stop_last=False):\n",
        "    \"\"\"\n",
        "    Retourne les k meilleurs successeurs selon P(.|context), avec filtres.\n",
        "    \"\"\"\n",
        "    forbid = {\"<BOS>\"}\n",
        "    if not allow_eos:\n",
        "        forbid.add(\"<EOS>\")\n",
        "    cands = []\n",
        "    for w in model.vocab:\n",
        "        if w in forbid:\n",
        "            continue\n",
        "        if not _is_word_like(w):\n",
        "            continue\n",
        "        if forbid_stop_last and (w in STOP_LAST or len(w) <= 2):\n",
        "            continue\n",
        "        p = model.score(w, context)\n",
        "        cands.append((p, w))\n",
        "    cands.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [w for _, w in cands[:k]]\n",
        "\n",
        "def fill_mask_run_with_beam(model, left_generated, run_len, remaining_after,\n",
        "                            beam_size=3, topk=8):\n",
        "    \"\"\"\n",
        "    Remplit un bloc de `run_len` MASKTOKEN consécutifs par beam search court.\n",
        "    Renvoie une liste de séquences candidates, ordonnées du meilleur au moins bon\n",
        "    selon la somme des log-probabilités locales.\n",
        "    \"\"\"\n",
        "    beams = [([], 0.0)]  # (sequence, logP)\n",
        "    for pos in range(run_len):\n",
        "        new_beams = []\n",
        "        for seq, logp in beams:\n",
        "            # Contexte gauche = déjà généré + séquence partielle du run\n",
        "            context = _left_context_with_bos(left_generated + seq, model.order)\n",
        "            # EOS seulement au tout dernier mot du run et s'il ne reste plus de masques après\n",
        "            allow_eos = (pos == run_len - 1) and (remaining_after == 0)\n",
        "            # Dernier mot du run: éviter de finir par un mot-outil court\n",
        "            forbid_stop_last = (pos == run_len - 1)\n",
        "\n",
        "            for w in top_k_next_words(model, context, k=topk,\n",
        "                                      allow_eos=allow_eos,\n",
        "                                      forbid_stop_last=forbid_stop_last):\n",
        "                p = model.score(w, context)\n",
        "                lp = math.log(p) if p > 0 else -1e9\n",
        "                new_beams.append((seq + [w], logp + lp))\n",
        "\n",
        "        if not new_beams:  # garde-fou\n",
        "            return [[\"xxx\"] * run_len]\n",
        "\n",
        "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "        beams = new_beams[:beam_size]\n",
        "\n",
        "    return [seq for (seq, _) in beams]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "ab666d7d",
      "metadata": {
        "id": "ab666d7d"
      },
      "outputs": [],
      "source": [
        "# Remplacement des masques\n",
        "def remove_masks_and_generate(model, masked_text):\n",
        "    \"\"\"\n",
        "    Remplace chaque run de MASKTOKEN par une petite recherche (beam) et\n",
        "    choisit la meilleure séquence via un re-ranking par perplexité globale.\n",
        "    Retourne (nb_masks, tokens_générés).\n",
        "    \"\"\"\n",
        "    safe_text = masked_text.lower().replace(\"***\", \"MASKTOKEN\")\n",
        "    tokens = word_tokenize(safe_text)\n",
        "\n",
        "    nb_masks = tokens.count(\"MASKTOKEN\")\n",
        "    generated = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        if tokens[i] != \"MASKTOKEN\":\n",
        "            generated.append(tokens[i])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # repérer un run de masques consécutifs\n",
        "        j = i\n",
        "        while j < len(tokens) and tokens[j] == \"MASKTOKEN\":\n",
        "            j += 1\n",
        "        run_len = j - i\n",
        "        remaining_after = tokens[j:].count(\"MASKTOKEN\")\n",
        "\n",
        "        # candidats via beam\n",
        "        candidates = fill_mask_run_with_beam(\n",
        "            model, generated, run_len, remaining_after,\n",
        "            beam_size=3, topk=8\n",
        "        )\n",
        "\n",
        "        # re-ranking par perplexité de la phrase ENTIEREMENT reconstruite\n",
        "        best_seq, best_pp = None, float(\"inf\")\n",
        "        for cand in candidates:\n",
        "            full = generated + cand + tokens[j:]  # gauche + cand + droite\n",
        "            pp = full_sentence_perplexity(model, full)\n",
        "            if pp < best_pp:\n",
        "                best_pp, best_seq = pp, cand\n",
        "\n",
        "        generated.extend(best_seq if best_seq is not None else candidates[0])\n",
        "        i = j\n",
        "\n",
        "    return nb_masks, generated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab440c3",
      "metadata": {
        "id": "8ab440c3"
      },
      "source": [
        "Mettre dans les prochaines cellules votre code pour évaluer les modèles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1314ad7",
      "metadata": {
        "id": "e1314ad7"
      },
      "source": [
        "Le sujet demande d’“estimer la proportion de mots générés qui correspondent au proverbe connu (accuracy)”.\n",
        "\n",
        "On propose donc deux métriques complémentaires :\n",
        "- Exact match (toute la phrase) — indicatif, mais sévère.\n",
        "- Accuracy “mots masqués” : on ne note que les mots générés aux positions masquées ; on compte la part correcte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "6601a4b9",
      "metadata": {
        "id": "6601a4b9"
      },
      "outputs": [],
      "source": [
        "# Métriques d'évaluation\n",
        "def masked_word_accuracy(pred_str, gold_str, masked_str):\n",
        "    \"\"\"\n",
        "    Calcule l'accuracy *sur les positions masquées seulement*.\n",
        "    - On localise les positions des MASKTOKEN dans la version tokenisée du 'masked_str'.\n",
        "    - On compare, sur ces positions, les tokens normalisés de 'pred_str' et de 'gold_str'.\n",
        "    Retourne (nb_mask_positions, nb_correct).\n",
        "    \"\"\"\n",
        "    # positions masquées dans la phrase de départ\n",
        "    masked_tokens = tokens_norm(masked_str.replace(\"***\", \"MASKTOKEN\")) #les astérisques risquent d’être séparés/jetés comme de la ponctuation\n",
        "    mask_positions = [i for i, t in enumerate(masked_tokens) if t == \"masktoken\"]\n",
        "\n",
        "    # tokens normalisés de pred/gold\n",
        "    pred_tokens = tokens_norm(pred_str)\n",
        "    gold_tokens = tokens_norm(gold_str)\n",
        "\n",
        "    # sécurité : si longueurs différentes, on prend les positions valides\n",
        "    m = 0\n",
        "    correct = 0\n",
        "    for pos in mask_positions:\n",
        "        if pos < len(pred_tokens) and pos < len(gold_tokens):\n",
        "            m += 1\n",
        "            if pred_tokens[pos] == gold_tokens[pos]:\n",
        "                correct += 1\n",
        "    return m, correct\n",
        "\n",
        "import math\n",
        "def evaluate_generation_exact_and_mask(tests, n=3):\n",
        "    \"\"\"\n",
        "    Évalue un modèle N-grammes (n) pour la complétion :\n",
        "      - exact match (phrase entière, via eq_by_tokens)\n",
        "      - accuracy mots masqués\n",
        "    Retourne un dict de métriques.\n",
        "    \"\"\"\n",
        "    model = models[n]\n",
        "    total = len(tests)\n",
        "    exact_correct = 0\n",
        "    mask_total = 0\n",
        "    mask_correct = 0\n",
        "\n",
        "    for test in tests:\n",
        "        masked = test[\"Masked\"]\n",
        "        gold = test[\"Proverb\"]\n",
        "        _, gen_tokens = remove_masks_and_generate(model, masked)\n",
        "        pred = \" \".join(gen_tokens)\n",
        "\n",
        "        # exact match\n",
        "        if eq_by_tokens(pred, gold):\n",
        "            exact_correct += 1\n",
        "\n",
        "        # mask-only accuracy\n",
        "        m, c = masked_word_accuracy(pred, gold, masked)\n",
        "        mask_total += m\n",
        "        mask_correct += c\n",
        "\n",
        "    return {\n",
        "        \"exact_correct\": exact_correct,\n",
        "        \"total\": total,\n",
        "        \"exact_acc\": exact_correct/total if total else 0.0,\n",
        "        \"mask_correct\": mask_correct,\n",
        "        \"mask_total\": mask_total,\n",
        "        \"mask_acc\": mask_correct/mask_total if mask_total else 0.0\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "394f1fef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "394f1fef",
        "outputId": "3ec2f990-217e-40a2-8e76-4386a852b007",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle 1-grammes :\n",
            "  Exact match      : 0/27  (acc = 0.00%)\n",
            "  Mots masqués     : 3/45  (acc = 6.67%)\n",
            "------------------------------------------------------------\n",
            "Modèle 2-grammes :\n",
            "  Exact match      : 1/27  (acc = 3.70%)\n",
            "  Mots masqués     : 6/45  (acc = 13.33%)\n",
            "------------------------------------------------------------\n",
            "Modèle 3-grammes :\n",
            "  Exact match      : 15/27  (acc = 55.56%)\n",
            "  Mots masqués     : 31/45  (acc = 68.89%)\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# résultats globaux\n",
        "for n in (1, 2, 3):\n",
        "    m = evaluate_generation_exact_and_mask(tests_generation, n=n)\n",
        "    print(f\"Modèle {n}-grammes :\")\n",
        "    print(f\"  Exact match      : {m['exact_correct']}/{m['total']}  (acc = {m['exact_acc']:.2%})\")\n",
        "    print(f\"  Mots masqués     : {m['mask_correct']}/{m['mask_total']}  (acc = {m['mask_acc']:.2%})\")\n",
        "    print(\"-\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "0a5ceaa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a5ceaa3",
        "outputId": "07621c03-d462-4d91-9adc-1064072523ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Modèle 1-grammes =====\n",
            "Test 1:\n",
            "  Masked  : a beau mentir qui vient de ***\n",
            "  Pred    : a beau mentir qui vient de fait\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 2:\n",
            "  Masked  : a beau mentir qui vient *** ***\n",
            "  Pred    : a beau mentir qui vient de fait\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 3:\n",
            "  Masked  : a beau mentir qui *** *** ***\n",
            "  Pred    : a beau mentir qui de de fait\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 4:\n",
            "  Masked  : l’occasion fait *** ***\n",
            "  Pred    : l ’ occasion fait de fait\n",
            "  Gold    : l’occasion fait le larron\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 5:\n",
            "  Masked  : aide-toi, le ciel t’***\n",
            "  Pred    : aide-toi , le ciel t ’ fait\n",
            "  Gold    : aide-toi, le ciel t’aidera\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 6:\n",
            "  Masked  : ce que femme veut, dieu le ***\n",
            "  Pred    : ce que femme veut , dieu le fait\n",
            "  Gold    : ce que femme veut, dieu le veut\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 7:\n",
            "  Masked  : ce que femme veut, dieu *** ***\n",
            "  Pred    : ce que femme veut , dieu de fait\n",
            "  Gold    : ce que femme veut, dieu le veut\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 8:\n",
            "  Masked  : bien mal acquis ne profite ***\n",
            "  Pred    : bien mal acquis ne profite fait\n",
            "  Gold    : bien mal acquis ne profite jamais\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 9:\n",
            "  Masked  : bon ouvrier ne querelle pas ses ***\n",
            "  Pred    : bon ouvrier ne querelle pas ses fait\n",
            "  Gold    : bon ouvrier ne querelle pas ses outils\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 10:\n",
            "  Masked  : bon ouvrier ne querelle *** *** ***\n",
            "  Pred    : bon ouvrier ne querelle de de fait\n",
            "  Gold    : bon ouvrier ne querelle pas ses outils\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 11:\n",
            "  Masked  : pour le fou, c’est tous les jours ***\n",
            "  Pred    : pour le fou , c ’ est tous les jours fait\n",
            "  Gold    : pour le fou, c’est tous les jours fête\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 12:\n",
            "  Masked  : dire et faire, sont ***\n",
            "  Pred    : dire et faire , sont fait\n",
            "  Gold    : dire et faire, sont deux\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 13:\n",
            "  Masked  : dire et faire, *** ***\n",
            "  Pred    : dire et faire , de fait\n",
            "  Gold    : dire et faire, sont deux\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 14:\n",
            "  Masked  : mieux vaut prévenir que ***\n",
            "  Pred    : mieux vaut prévenir que fait\n",
            "  Gold    : mieux vaut prévenir que guérir\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 15:\n",
            "  Masked  : mieux vaut prévenir *** ***\n",
            "  Pred    : mieux vaut prévenir de fait\n",
            "  Gold    : mieux vaut prévenir que guérir\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 16:\n",
            "  Masked  : à qui dieu aide, nul ne peut ***\n",
            "  Pred    : à qui dieu aide , nul ne peut fait\n",
            "  Gold    : à qui dieu aide, nul ne peut nuire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 17:\n",
            "  Masked  : à qui dieu aide, nul ne *** ***\n",
            "  Pred    : à qui dieu aide , nul ne de fait\n",
            "  Gold    : à qui dieu aide, nul ne peut nuire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 18:\n",
            "  Masked  : il faut le voir pour le ***\n",
            "  Pred    : il faut le voir pour le fait\n",
            "  Gold    : il faut le voir pour le croire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 19:\n",
            "  Masked  : il faut le voir *** *** ***\n",
            "  Pred    : il faut le voir de de fait\n",
            "  Gold    : il faut le voir pour le croire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 20:\n",
            "  Masked  : on ne vend pas le poisson qui est encore dans la ***\n",
            "  Pred    : on ne vend pas le poisson qui est encore dans la fait\n",
            "  Gold    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 21:\n",
            "  Masked  : on ne vend pas le poisson qui est encore *** *** ***\n",
            "  Pred    : on ne vend pas le poisson qui est encore de de fait\n",
            "  Gold    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 22:\n",
            "  Masked  : le poisson pourrit par la ***\n",
            "  Pred    : le poisson pourrit par la fait\n",
            "  Gold    : le poisson pourrit par la tête\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 23:\n",
            "  Masked  : couche-toi plutôt sans souper, que de te lever avec des ***\n",
            "  Pred    : couche-toi plutôt sans souper , que de te lever avec des fait\n",
            "  Gold    : couche-toi plutôt sans souper, que de te lever avec des dettes\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 24:\n",
            "  Masked  : couche-toi plutôt sans souper, que de te lever *** *** ***\n",
            "  Pred    : couche-toi plutôt sans souper , que de te lever de de fait\n",
            "  Gold    : couche-toi plutôt sans souper, que de te lever avec des dettes\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 25:\n",
            "  Masked  : les montagnes ne se rencontrent point, mais les hommes se ***\n",
            "  Pred    : les montagnes ne se rencontrent point , mais les hommes se fait\n",
            "  Gold    : les montagnes ne se rencontrent point, mais les hommes se rencontrent\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 26:\n",
            "  Masked  : les montagnes ne se rencontrent point, mais les hommes *** ***\n",
            "  Pred    : les montagnes ne se rencontrent point , mais les hommes de fait\n",
            "  Gold    : les montagnes ne se rencontrent point, mais les hommes se rencontrent\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 27:\n",
            "  Masked  : manger peu, chasse beaucoup *** ***\n",
            "  Pred    : manger peu , chasse beaucoup de fait\n",
            "  Gold    : manger peu, chasse beaucoup de maladies\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "\n",
            "===== Modèle 2-grammes =====\n",
            "Test 1:\n",
            "  Masked  : a beau mentir qui vient de ***\n",
            "  Pred    : a beau mentir qui vient de ton\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 2:\n",
            "  Masked  : a beau mentir qui vient *** ***\n",
            "  Pred    : a beau mentir qui vient à demi\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 3:\n",
            "  Masked  : a beau mentir qui *** *** ***\n",
            "  Pred    : a beau mentir qui ne se fait\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 4:\n",
            "  Masked  : l’occasion fait *** ***\n",
            "  Pred    : l ’ occasion fait le monde\n",
            "  Gold    : l’occasion fait le larron\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 5:\n",
            "  Masked  : aide-toi, le ciel t’***\n",
            "  Pred    : aide-toi , le ciel t ’ homme\n",
            "  Gold    : aide-toi, le ciel t’aidera\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 6:\n",
            "  Masked  : ce que femme veut, dieu le ***\n",
            "  Pred    : ce que femme veut , dieu le monde\n",
            "  Gold    : ce que femme veut, dieu le veut\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 7:\n",
            "  Masked  : ce que femme veut, dieu *** ***\n",
            "  Pred    : ce que femme veut , dieu ne faut\n",
            "  Gold    : ce que femme veut, dieu le veut\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 8:\n",
            "  Masked  : bien mal acquis ne profite ***\n",
            "  Pred    : bien mal acquis ne profite beaucoup\n",
            "  Gold    : bien mal acquis ne profite jamais\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 9:\n",
            "  Masked  : bon ouvrier ne querelle pas ses ***\n",
            "  Pred    : bon ouvrier ne querelle pas ses fautes\n",
            "  Gold    : bon ouvrier ne querelle pas ses outils\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 10:\n",
            "  Masked  : bon ouvrier ne querelle *** *** ***\n",
            "  Pred    : bon ouvrier ne querelle pas de ton\n",
            "  Gold    : bon ouvrier ne querelle pas ses outils\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 11:\n",
            "  Masked  : pour le fou, c’est tous les jours ***\n",
            "  Pred    : pour le fou , c ’ est tous les jours pâques\n",
            "  Gold    : pour le fou, c’est tous les jours fête\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 12:\n",
            "  Masked  : dire et faire, sont ***\n",
            "  Pred    : dire et faire , sont deux\n",
            "  Gold    : dire et faire, sont deux\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 13:\n",
            "  Masked  : dire et faire, *** ***\n",
            "  Pred    : dire et faire , il faut\n",
            "  Gold    : dire et faire, sont deux\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 14:\n",
            "  Masked  : mieux vaut prévenir que ***\n",
            "  Pred    : mieux vaut prévenir que par\n",
            "  Gold    : mieux vaut prévenir que guérir\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 15:\n",
            "  Masked  : mieux vaut prévenir *** ***\n",
            "  Pred    : mieux vaut prévenir que par\n",
            "  Gold    : mieux vaut prévenir que guérir\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 16:\n",
            "  Masked  : à qui dieu aide, nul ne peut ***\n",
            "  Pred    : à qui dieu aide , nul ne peut être\n",
            "  Gold    : à qui dieu aide, nul ne peut nuire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 17:\n",
            "  Masked  : à qui dieu aide, nul ne *** ***\n",
            "  Pred    : à qui dieu aide , nul ne se fait\n",
            "  Gold    : à qui dieu aide, nul ne peut nuire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 18:\n",
            "  Masked  : il faut le voir pour le ***\n",
            "  Pred    : il faut le voir pour le monde\n",
            "  Gold    : il faut le voir pour le croire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 19:\n",
            "  Masked  : il faut le voir *** *** ***\n",
            "  Pred    : il faut le voir que le monde\n",
            "  Gold    : il faut le voir pour le croire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 20:\n",
            "  Masked  : on ne vend pas le poisson qui est encore dans la ***\n",
            "  Pred    : on ne vend pas le poisson qui est encore dans la mort\n",
            "  Gold    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 21:\n",
            "  Masked  : on ne vend pas le poisson qui est encore *** *** ***\n",
            "  Pred    : on ne vend pas le poisson qui est encore mieux vaut mieux\n",
            "  Gold    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 22:\n",
            "  Masked  : le poisson pourrit par la ***\n",
            "  Pred    : le poisson pourrit par la mort\n",
            "  Gold    : le poisson pourrit par la tête\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 23:\n",
            "  Masked  : couche-toi plutôt sans souper, que de te lever avec des ***\n",
            "  Pred    : couche-toi plutôt sans souper , que de te lever avec des autres\n",
            "  Gold    : couche-toi plutôt sans souper, que de te lever avec des dettes\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 24:\n",
            "  Masked  : couche-toi plutôt sans souper, que de te lever *** *** ***\n",
            "  Pred    : couche-toi plutôt sans souper , que de te lever de la mort\n",
            "  Gold    : couche-toi plutôt sans souper, que de te lever avec des dettes\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 25:\n",
            "  Masked  : les montagnes ne se rencontrent point, mais les hommes se ***\n",
            "  Pred    : les montagnes ne se rencontrent point , mais les hommes se fait\n",
            "  Gold    : les montagnes ne se rencontrent point, mais les hommes se rencontrent\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 26:\n",
            "  Masked  : les montagnes ne se rencontrent point, mais les hommes *** ***\n",
            "  Pred    : les montagnes ne se rencontrent point , mais les hommes de ton\n",
            "  Gold    : les montagnes ne se rencontrent point, mais les hommes se rencontrent\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 27:\n",
            "  Masked  : manger peu, chasse beaucoup *** ***\n",
            "  Pred    : manger peu , chasse beaucoup de ton\n",
            "  Gold    : manger peu, chasse beaucoup de maladies\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "\n",
            "===== Modèle 3-grammes =====\n",
            "Test 1:\n",
            "  Masked  : a beau mentir qui vient de ***\n",
            "  Pred    : a beau mentir qui vient de rome\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 2:\n",
            "  Masked  : a beau mentir qui vient *** ***\n",
            "  Pred    : a beau mentir qui vient de rome\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 3:\n",
            "  Masked  : a beau mentir qui *** *** ***\n",
            "  Pred    : a beau mentir qui vient de rome\n",
            "  Gold    : a beau mentir qui vient de loin\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 4:\n",
            "  Masked  : l’occasion fait *** ***\n",
            "  Pred    : l ’ occasion fait le larron\n",
            "  Gold    : l’occasion fait le larron\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 5:\n",
            "  Masked  : aide-toi, le ciel t’***\n",
            "  Pred    : aide-toi , le ciel t ’ aidera\n",
            "  Gold    : aide-toi, le ciel t’aidera\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 6:\n",
            "  Masked  : ce que femme veut, dieu le ***\n",
            "  Pred    : ce que femme veut , dieu le veut\n",
            "  Gold    : ce que femme veut, dieu le veut\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 7:\n",
            "  Masked  : ce que femme veut, dieu *** ***\n",
            "  Pred    : ce que femme veut , dieu fait trouver\n",
            "  Gold    : ce que femme veut, dieu le veut\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 8:\n",
            "  Masked  : bien mal acquis ne profite ***\n",
            "  Pred    : bien mal acquis ne profite jamais\n",
            "  Gold    : bien mal acquis ne profite jamais\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 9:\n",
            "  Masked  : bon ouvrier ne querelle pas ses ***\n",
            "  Pred    : bon ouvrier ne querelle pas ses outils\n",
            "  Gold    : bon ouvrier ne querelle pas ses outils\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 10:\n",
            "  Masked  : bon ouvrier ne querelle *** *** ***\n",
            "  Pred    : bon ouvrier ne querelle pas ses outils\n",
            "  Gold    : bon ouvrier ne querelle pas ses outils\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 11:\n",
            "  Masked  : pour le fou, c’est tous les jours ***\n",
            "  Pred    : pour le fou , c ’ est tous les jours fête\n",
            "  Gold    : pour le fou, c’est tous les jours fête\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 12:\n",
            "  Masked  : dire et faire, sont ***\n",
            "  Pred    : dire et faire , sont deux\n",
            "  Gold    : dire et faire, sont deux\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 13:\n",
            "  Masked  : dire et faire, *** ***\n",
            "  Pred    : dire et faire , et non\n",
            "  Gold    : dire et faire, sont deux\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 14:\n",
            "  Masked  : mieux vaut prévenir que ***\n",
            "  Pred    : mieux vaut prévenir que guérir\n",
            "  Gold    : mieux vaut prévenir que guérir\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 15:\n",
            "  Masked  : mieux vaut prévenir *** ***\n",
            "  Pred    : mieux vaut prévenir que guérir\n",
            "  Gold    : mieux vaut prévenir que guérir\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 16:\n",
            "  Masked  : à qui dieu aide, nul ne peut ***\n",
            "  Pred    : à qui dieu aide , nul ne peut mentir\n",
            "  Gold    : à qui dieu aide, nul ne peut nuire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 17:\n",
            "  Masked  : à qui dieu aide, nul ne *** ***\n",
            "  Pred    : à qui dieu aide , nul ne peut mentir\n",
            "  Gold    : à qui dieu aide, nul ne peut nuire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 18:\n",
            "  Masked  : il faut le voir pour le ***\n",
            "  Pred    : il faut le voir pour le fait\n",
            "  Gold    : il faut le voir pour le croire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 19:\n",
            "  Masked  : il faut le voir *** *** ***\n",
            "  Pred    : il faut le voir pour le fait\n",
            "  Gold    : il faut le voir pour le croire\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 20:\n",
            "  Masked  : on ne vend pas le poisson qui est encore dans la ***\n",
            "  Pred    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Gold    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 21:\n",
            "  Masked  : on ne vend pas le poisson qui est encore *** *** ***\n",
            "  Pred    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Gold    : on ne vend pas le poisson qui est encore dans la mer\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 22:\n",
            "  Masked  : le poisson pourrit par la ***\n",
            "  Pred    : le poisson pourrit par la tête\n",
            "  Gold    : le poisson pourrit par la tête\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 23:\n",
            "  Masked  : couche-toi plutôt sans souper, que de te lever avec des ***\n",
            "  Pred    : couche-toi plutôt sans souper , que de te lever avec des dattes\n",
            "  Gold    : couche-toi plutôt sans souper, que de te lever avec des dettes\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 24:\n",
            "  Masked  : couche-toi plutôt sans souper, que de te lever *** *** ***\n",
            "  Pred    : couche-toi plutôt sans souper , que de te lever avec des dattes\n",
            "  Gold    : couche-toi plutôt sans souper, que de te lever avec des dettes\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n",
            "Test 25:\n",
            "  Masked  : les montagnes ne se rencontrent point, mais les hommes se ***\n",
            "  Pred    : les montagnes ne se rencontrent point , mais les hommes se rencontrent\n",
            "  Gold    : les montagnes ne se rencontrent point, mais les hommes se rencontrent\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 26:\n",
            "  Masked  : les montagnes ne se rencontrent point, mais les hommes *** ***\n",
            "  Pred    : les montagnes ne se rencontrent point , mais les hommes se rencontrent\n",
            "  Gold    : les montagnes ne se rencontrent point, mais les hommes se rencontrent\n",
            "  Correct ? True\n",
            "------------------------------------------------------------\n",
            "Test 27:\n",
            "  Masked  : manger peu, chasse beaucoup *** ***\n",
            "  Pred    : manger peu , chasse beaucoup de paix\n",
            "  Gold    : manger peu, chasse beaucoup de maladies\n",
            "  Correct ? False\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Affichage détaillé\n",
        "def print_generation_results(tests, n=3, limit=None):\n",
        "    \"\"\"\n",
        "    Affiche pour le modèle n-grammes (n=1/2/3) :\n",
        "      - Masked (entrée)\n",
        "      - Pred (prédiction)\n",
        "      - Gold (référence)\n",
        "      - Correct ? (exact match, via eq_by_tokens)\n",
        "    \"\"\"\n",
        "    model = models[n]\n",
        "    total = len(tests)\n",
        "    to_show = total if limit is None else min(limit, total)\n",
        "\n",
        "    print(f\"\\n===== Modèle {n}-grammes =====\")\n",
        "    for i in range(to_show):\n",
        "        masked = tests[i][\"Masked\"]\n",
        "        gold = tests[i][\"Proverb\"]\n",
        "\n",
        "        _, gen_tokens = remove_masks_and_generate(model, masked)\n",
        "        pred = \" \".join(gen_tokens)\n",
        "\n",
        "        ok = eq_by_tokens(pred, gold)\n",
        "        print(f\"Test {i+1}:\")\n",
        "        print(f\"  Masked  : {masked}\")\n",
        "        print(f\"  Pred    : {pred}\")\n",
        "        print(f\"  Gold    : {gold}\")\n",
        "        print(f\"  Correct ? {ok}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# Exemple : tout afficher\n",
        "print_generation_results(tests_generation, n=1)\n",
        "print_generation_results(tests_generation, n=2)\n",
        "print_generation_results(tests_generation, n=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5720e938",
      "metadata": {
        "id": "5720e938"
      },
      "source": [
        "Décrivez les résultats obtenus et répondez aux questions dans l'énoncé du travail. Vous pouvez ajouter le nombre de cellules que vous souhaitez."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c7480d",
      "metadata": {
        "id": "72c7480d"
      },
      "source": [
        "### Résultats obtenus\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5205674",
      "metadata": {
        "id": "d5205674"
      },
      "source": [
        "**Exact match (phrase complète)**\n",
        "- 1-gramme : 0/27 (0.00 %)\n",
        "- 2-grammes : 1/27 (3.70 %)\n",
        "- 3-grammes : 15/27 (55.56 %)\n",
        "\n",
        "**Accuracy “mots masqués”**\n",
        "\n",
        "Donne une vision plus fine de la capacité à récupérer les bons mots aux positions masquées, même si toute la phrase n’est pas identique\n",
        "- 1-gramme : 3/45  (acc = 6.67%)\n",
        "- 2-grammes : 6/45  (acc = 13.33%)\n",
        "- 3-grammes : 31/45  (acc = 68.89%)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c497c977",
      "metadata": {
        "id": "c497c977"
      },
      "source": [
        "### Analyse\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0957c0dc",
      "metadata": {
        "id": "0957c0dc"
      },
      "source": [
        "Les résultats montrent un contraste très net entre les trigrammes et les deux autres ordres :\n",
        "- avec N=3, plus d’une phrase sur deux est reconstruite exactement et environ deux tiers des mots masqués sont corrects, alors que les unigrammes et bigrammes restent très faibles.\n",
        "- Ce comportement n’est ni accidentel ni “buggué” : il découle directement de la manière dont les N-grammes estiment les probabilités dans un corpus petit et très formulairisé comme un recueil de proverbes.\n",
        "\n",
        "Commençons par expliquer pourquoi l’unigramme propose des mots “vides” ou inadaptés :\n",
        "- Un modèle à N=1 ne conditionne jamais sur le contexte : la probabilité d’un mot ne dépend que de sa fréquence globale dans tout le corpus. Sur un ensemble de proverbes, cette distribution est dominée par des mots-outils – prépositions, déterminants, auxiliaires – parce qu’ils apparaissent partout, dans presque toutes les phrases. Quand on demande à l’unigramme de “deviner” un masque dans un contexte tel que « vient de ___ », il n’a aucun moyen de savoir que la suite attendue est “loin” plutôt que “fait” ou “plus” : pour lui, ces trois mots se comparent uniquement par leur fréquence totale. Le mécanisme de lissage de Laplace, que l’on utilise pour éviter les zéros de probabilité, accentue encore cette tendance en redistribuant une petite masse vers tous les mots non vus, ce qui a pour effet secondaire d’aplatir les contrastes et de rapprocher encore davantage le modèle d’un “prior de fréquence” global. Le résultat est logique : on obtient très souvent des sorties grammaticalement stables à l’échelle du corpus (des mots fréquents) mais localement absurdes dans la phrase. D’où les fins du type « … de fait » ou, si l’on ne filtre pas, « … de de ».\n",
        "\n",
        "- Le bigramme ajoute une information : le prochain mot est conditionné sur le mot précédent. En théorie, cela devrait déjà aider. En pratique, l’amélioration reste modeste pour deux raisons. D’abord, un seul mot de contexte laisse subsister une ambiguïté massive. La suite de « vient de __ » est structurellement très ouverte dans la langue : “loin”, “Rome”, “la mer”, “bon cœur”, “ce pays”, etc. Dans un petit corpus de ~3 000 proverbes, un grand nombre de ces continuations possibles n’apparaissent soit qu’une fois, soit pas du tout. Les comptes sont faibles, donc les estimations de probabilité conditionnelle deviennent instables ; et Laplace, en “comblant” les cases vides, réduit les écarts au profit de mots globalement fréquents. Ensuite, et surtout, le bigramme ne “voit” pas ce qui se passe deux mots en arrière. Or, de nombreuses terminaisons proverbiales sont des collocations de longueur au moins trois (“fait le larron”, “dans la mer”, “par la tête”). Avec seulement un mot d’historique, le modèle ne peut pas distinguer « vient de __ » dans le proverbe “a beau mentir qui vient de loin” d’un autre « vient de ___ » quelconque. Il bascule alors vers des bigrammes fréquents mais hors-sens (“t’ homme”, “que par”, “le monde”), parce que ce sont ceux dont les comptes sont les moins rares dans un si petit corpus. Autrement dit, l’incertitude contextuelle et la sparsité des observations entraînent le bigramme à “se rabattre” sur des solutions faciles et fréquentes, qui donnent mécaniquement des mots inadaptés.\n",
        "\n",
        "- Le trigramme change la donne parce qu’il conditionne sur deux mots de contexte. Ce simple pas supplémentaire suffit à réactiver un grand nombre de patrons proverbiaux figés. Dès que l’historique contient « fait le », « prévenir que », « dans la », « par la », « sont deux », le modèle retrouve une continuité très fortement corrélée à la bonne réponse. On le voit dans les reconstructions : “le larron”, “que guérir”, “dans la mer”, “par la tête”, “sont deux” redeviennent probables. La stratégie de décodage utilisée – une petite recherche en faisceau suivie d’un re-classement final par perplexité de la phrase complète – consolide encore ce signal : plutôt que de s’enfermer dans un choix local myope, on évalue la fluidité globale, avec le padding BOS/EOS et les n-grammes de la phrase entière. C’est précisément ce qui permet au trigramme de dépasser la barre symbolique d’une phrase sur deux parfaitement reconstruite.\n",
        "\n",
        "Pour autant, le trigramme n’est pas parfait, et là encore, la cause est structurelle. Avec un corpus de taille modeste, beaucoup de trigrammes décisifs n’apparaissent qu’une poignée de fois, voire jamais. Le lissage de Laplace évite les probabilités nulles mais tend à “sur-lisser” : en mettant un peu de masse partout, il atténue la force des collocations rares pourtant hautement discriminantes en fin de proverbe. Le modèle hésite alors entre des fins proches sémantiquement mais non proverbiales, par exemple « mentir » au lieu de « nuire », « main » au lieu de « mer », « fait » au lieu de « croire ». Ce ne sont pas des “erreurs absurdes”, ce sont des glissements typiques d’un modèle local peu informé au-delà de deux mots, confronté à une forte sparsité et à un lissage uniforme. Notre décodage aide, mais il ne peut pas compenser les limites intrinsèques du modèle : s’il manque d’évidence statistique, optimiser la recherche ne créera pas de signal.\n",
        "\n",
        "Enfin, quelques remarques sur les métriques éclairent la lecture des scores. L’exact match au niveau de la phrase est volontairement strict : une seule erreur à une position masquée invalide l’ensemble, ce qui explique le faible rendement des N=1 et N=2 et le plafond du N=3. La mesure “accuracy sur mots masqués” isole la capacité locale à choisir le bon mot ; c’est là que l’on voit le trigramme culminer autour des deux tiers : lorsque le contexte de deux mots porte suffisamment d’information, la suite proverbiale redevient la plus probable ; mais dès que l’on sort des patrons fréquents, la sparsité et le lissage reprennent le dessus.\n",
        "\n",
        "En somme, les “mots absurdes\" en unigramme et, dans une moindre mesure, en bigramme ne sont pas un symptôme d’implémentation défaillante : ils sont la manifestation attendue d’un modèle qui, faute de contexte ou avec un contexte trop court, retombe sur le biais de fréquence globale et des bigrammes génériques. Le trigramme franchit un cap parce qu’il recolle aux collocations figées qui caractérisent la langue des proverbes ; mais il reste borné par la taille réduite du corpus et par un lissage (Laplace) qui aplatit les contrastes. LA longueur d’historique est le levier déterminant ; la sparsité et le lissage expliquent les plafonds ; et le décodage par beam + re-ranking à la perplexité améliore sensiblement N=3 sans pouvoir transcender les limites statistiques des modèles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcba2d5a",
      "metadata": {
        "id": "bcba2d5a"
      },
      "source": [
        "## Section 5 - Partie réservée pour faire nos tests lors de la correction\n",
        "\n",
        "Merci de ne pas modifier ni retirer cette section du notebook.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290fd2c8",
      "metadata": {
        "id": "290fd2c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b046259",
      "metadata": {
        "id": "9b046259"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e25cbd4",
      "metadata": {
        "id": "7e25cbd4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
